---
title: "Reticulated SageMaker Tutorial"
date: "1/24/2019"
output: html_document
---

This Rmd contains a summary of the tutorial steps prodvided in [Using R with Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/using-r-with-amazon-sagemaker/) writen by **Ryan Garner** and published on the **AWS Machine Learning Blog** on May 29, 2018. Please visit the original blog to read more about Amazon SageMaker and integrations with R.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Reticulating the Amazon SageMaker Python SDK

Use the reticulate package to import sagemaker:

```{r}
library(reticulate)
sagemaker <- import('sagemaker')
session <- sagemaker$Session()
```

## Creating and accessing the data storage

Create an S3 bucket - this will store the training data, model binary file and output from the training:

```{r}
role_arn <- session$expand_role('sagemaker-service-role')
bucket <- session$default_bucket()
```

## Downloading and processing the dataset

```{r}
library(readr)
data_file <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'
abalone <- read_csv(file = data_file, col_names = FALSE)
names(abalone) <- c('sex', 'length', 'diameter', 'height', 'whole_weight', 'shucked_weight', 'viscera_weight', 'shell_weight', 'rings')
head(abalone)
```

Change `sex` to a factor and view a data summary using the `skimr` package:

```{r}
library(skimr)
abalone$sex <- as.factor(abalone$sex)
skim(abalone)
```

Visually explore the data with `ggplot2`:

```{r}
library(ggplot2)
ggplot(abalone, aes(x = height, y = rings, color = sex)) + geom_point() + geom_jitter()
```

Filter the two abalones with height 0 using the `dplyr` package:

```{r}
library(dplyr)
abalone <- abalone %>%
  filter(height != 0)
```

## Preparing the dataset for model training

Create three datasets: training, testing and validation
- Convert `sex` into a dummy variable
- Move the target variable `rings` to the first column

_Amazon SageMaker algorithms require the target to be in the first column of the dataset._

```{r}
abalone <- abalone %>%
  mutate(female = as.integer(ifelse(sex == 'F', 1, 0)),
         male = as.integer(ifelse(sex == 'M', 1, 0)),
         infant = as.integer(ifelse(sex == 'I', 1, 0))) %>%
  select(-sex)
abalone <- abalone %>%
  select(rings:infant, length:shell_weight)
head(abalone)
```

Sample and split to create three datasets:

```{r}
abalone_train <- abalone %>%
  sample_frac(size = 0.7)
abalone <- anti_join(abalone, abalone_train)
abalone_test <- abalone %>%
  sample_frac(size = 0.5)
abalone_valid <- anti_join(abalone, abalone_test)
```

Save the training and validation sets locally and upload them to the S3 bucket:

```{r}
write_csv(abalone_train, 'abalone_train.csv', col_names = FALSE)
write_csv(abalone_valid, 'abalone_valid.csv', col_names = FALSE)

s3_train <- session$upload_data(path = 'abalone_train.csv',
                                bucket = bucket,
                                key_prefix = 'data')
s3_valid <- session$upload_data(path = 'abalone_valid.csv',
                                bucket = bucket,
                                key_prefix = 'data')
```

Define S3 input types for Amazon SageMaker:  

```{r}
s3_train_input <- sagemaker$s3_input(s3_data = s3_train,
                                     content_type = 'csv')
s3_valid_input <- sagemaker$s3_input(s3_data = s3_valid,
                                     content_type = 'csv')
```

## Training the model

Specify Docker containers in Amazon ECR for training an XGBoost model:

```{r}
containers <- list('us-west-2' = '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',
  'us-east-1' = '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',
  'us-east-2' = '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',
  'eu-west-1' = '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest')
container <- containers[session$boto_region_name][[1]]
```

Define the SageMaker Estimator:

```{r}
s3_output <- paste0('s3://', bucket, '/output')
estimator <- sagemaker$estimator$Estimator(image_name = container,
                                           role = role_arn,
                                           train_instance_count = 1L,
                                           train_instance_type = 'ml.m5.large',
                                           train_volume_size = 30L,
                                           train_max_run = 3600L,
                                           input_mode = 'File',
                                           output_path = s3_output,
                                           output_kms_key = NULL,
                                           base_job_name = NULL,
                                           sagemaker_session = NULL)
```

Specify the XGBoost hyperparameters:

```{r}
estimator$set_hyperparameters(num_round = 100L)
job_name <- paste('sagemaker-train-xgboost', format(Sys.time(), '%H-%M-%S'), sep = '-')
input_data <- list('train' = s3_train_input,
                   'validation' = s3_valid_input)
estimator$fit(inputs = input_data,
              job_name = job_name)

# Print the S3 path for the model binary
estimator$model_data
```

## Deploying the model

Deploy the trained model to an `ml.t2.medium` instance:

```{r}
model_endpoint <- estimator$deploy(initial_instance_count = 1L,
                                   instance_type = 'ml.t2.medium')
```

## Generating predictions with the model

Use the test data to generate predictions:

```{r}
model_endpoint$content_type <- 'text/csv'
model_endpoint$serializer <- sagemaker$predictor$csv_serializer

# Remove the target column, convert row selection to a matrix
abalone_test <- abalone_test[-1]
num_predict_rows <- 500
test_sample <- as.matrix(abalone_test[1:num_predict_rows, ])
dimnames(test_sample)[[2]] <- NULL

# Generate predictions
library(stringr)
predictions <- model_endpoint$predict(test_sample)
predictions <- str_split(predictions, pattern = ',', simplify = TRUE)
predictions <- as.numeric(predictions)
abalone_test <- cbind(predicted_rings = predictions,
                      abalone_test[1:num_predict_rows, ])
head(abalone_test)
```

## Deleting the endpoint

**This is an important step!**

When youâ€™re done with the model, delete the endpoint to avoid incurring deployment costs:

```{r}
session$delete_endpoint(model_endpoint$endpoint)
```
